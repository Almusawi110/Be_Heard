{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ed011-85ef-4d70-9829-57ed4096e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNR\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from itertools import product\n",
    "import keyboard\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Draw the landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        results: The landmarks detected by Mediapipe.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the image is writable before drawing landmarks\n",
    "    image.flags.writeable = True\n",
    "    # Draw landmarks for left hand\n",
    "    if results.left_hand_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image, results.left_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "    # Draw landmarks for right hand\n",
    "    if results.right_hand_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image, results.right_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "\n",
    "\n",
    "def image_process(image, model):\n",
    "    \"\"\"\n",
    "    Process the image and obtain sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        model: The Mediapipe holistic object.\n",
    "\n",
    "    Returns:\n",
    "        results: The processed results containing sign landmarks.\n",
    "    \"\"\"\n",
    "    # Set the image to read-only mode\n",
    "    image.flags.writeable = False\n",
    "    # Convert the image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image using the model\n",
    "    results = model.process(image)\n",
    "    # Set the image back to writeable mode\n",
    "    image.flags.writeable = True\n",
    "    # Convert the image back from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return results\n",
    "\n",
    "def keypoint_extraction(results):\n",
    "    \"\"\"\n",
    "    Extract the keypoints from the sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        results: The processed results containing sign landmarks.\n",
    "\n",
    "    Returns:\n",
    "        keypoints (numpy.ndarray): The extracted keypoints.\n",
    "    \"\"\"\n",
    "    # Extract the keypoints for the left hand if present, otherwise set to zeros\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    # Extract the keypoints for the right hand if present, otherwise set to zeros\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    # Concatenate the keypoints for both hands\n",
    "    keypoints = np.concatenate([lh, rh])\n",
    "\n",
    "# Define the actions (signs) that will be recorded and stored in the dataset\n",
    "actions = np.array(['Hello','Please','Sorry','No','yes','ILoveYou','Thankyou'])\n",
    "\n",
    "# Define the number of sequences and frames to be recorded for each action\n",
    "sequences = 30\n",
    "frames = 10\n",
    "\n",
    "# Set the path where the dataset will be stored\n",
    "PATH = os.path.join(r'C:\\Harosha\\ML Projects\\Sign_Language_Detector\\Images\\Collected_Images')\n",
    "\n",
    "# Create directories for each action, sequence, and frame in the dataset\n",
    "for action, sequence in product(actions, range(sequences)):\n",
    "    try:\n",
    "        os.makedirs(os.path.join(PATH, action, str(sequence)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Access the camera and check if the camera is opened successfully\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot access camera.\")\n",
    "    exit()\n",
    "\n",
    "# Create a MediaPipe Holistic object for hand tracking and landmark extraction\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "    # Loop through each action, sequence, and frame to record data\n",
    "    for action, sequence, frame in product(actions, range(sequences), range(frames)):\n",
    "        # If it is the first frame of a sequence, wait for the spacebar key press to start recording\n",
    "        if frame == 0: \n",
    "            while True:\n",
    "                if keyboard.is_pressed(' '):\n",
    "                    break\n",
    "                _, image = cap.read()\n",
    "\n",
    "                results = image_process(image, holistic)\n",
    "                draw_landmarks(image, results)\n",
    "\n",
    "                # Ensure the image is writable before drawing text\n",
    "                image.flags.writeable = True\n",
    "\n",
    "                cv2.putText(image, f'Recording data for \"{action}\". Sequence number {sequence}.',\n",
    "                            (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Pause.', (20, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Press \"Space\" when you are ready.', (20, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('Camera', image)\n",
    "                cv2.waitKey(1)\n",
    "                \n",
    "                # Check if the 'Camera' window was closed and break the loop\n",
    "                if cv2.getWindowProperty('Camera', cv2.WND_PROP_VISIBLE) < 1:\n",
    "                    break\n",
    "        else:\n",
    "            # For subsequent frames, directly read the image from the camera\n",
    "            _, image = cap.read()\n",
    "            # Process the image and extract hand landmarks using the MediaPipe Holistic pipeline\n",
    "            results = image_process(image, holistic)\n",
    "            # Draw the hand landmarks on the image\n",
    "            draw_landmarks(image, results)\n",
    "\n",
    "            # Ensure the image is writable before drawing text\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            # Display text on the image indicating the action and sequence number being recorded\n",
    "            cv2.putText(image, f'Recording data for \"{action}\". Sequence number {sequence}.',\n",
    "                        (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('Camera', image)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        # Check if the 'Camera' window was closed and break the loop\n",
    "        if cv2.getWindowProperty('Camera', cv2.WND_PROP_VISIBLE) < 1:\n",
    "             break\n",
    "\n",
    "        # Extract the landmarks from both hands and save them in arrays\n",
    "        keypoints = keypoint_extraction(results)\n",
    "        frame_path = os.path.join(PATH, action, str(sequence), str(frame))\n",
    "        np.save(frame_path, keypoints)\n",
    "\n",
    "# Release the camera and close any remaining windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15326539-1764-4ff5-b594-b0a242d54afd",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815253a1-7aac-41c2-b8ed-a77626cbca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - categorical_accuracy: 0.1748 - loss: 1.9449\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.2981 - loss: 1.9320 \n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.2326 - loss: 1.9126 \n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.2434 - loss: 1.8578 \n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.2411 - loss: 1.7537 \n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.2873 - loss: 1.5890 \n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.4102 - loss: 1.4002 \n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.5831 - loss: 1.1063 \n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.4934 - loss: 1.1368 \n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.5836 - loss: 0.9647 \n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.5549 - loss: 0.9997 \n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.6513 - loss: 0.8923 \n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.6564 - loss: 0.9148 \n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7356 - loss: 0.7467 \n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7358 - loss: 0.6819 \n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7677 - loss: 0.6656 \n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7058 - loss: 0.7251 \n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7720 - loss: 0.7043 \n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7574 - loss: 0.6849 \n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.7391 - loss: 0.7171 \n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7385 - loss: 0.6976 \n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.7330 - loss: 0.6846 \n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8020 - loss: 0.5922 \n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8280 - loss: 0.4877 \n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8208 - loss: 0.4926 \n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8543 - loss: 0.4865 \n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.7287 - loss: 0.5931 \n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8027 - loss: 0.5023 \n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8311 - loss: 0.5210 \n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8511 - loss: 0.4341 \n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8122 - loss: 0.4421 \n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8472 - loss: 0.4349 \n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8676 - loss: 0.3746 \n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8755 - loss: 0.3426 \n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8921 - loss: 0.3544 \n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9008 - loss: 0.3259 \n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8782 - loss: 0.3635 \n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8448 - loss: 0.4425 \n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8629 - loss: 0.3824 \n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - categorical_accuracy: 0.8679 - loss: 0.3532\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8988 - loss: 0.2565 \n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9219 - loss: 0.2589 \n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8758 - loss: 0.2840 \n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9051 - loss: 0.2729 \n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9270 - loss: 0.2478 \n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8372 - loss: 0.5053 \n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8474 - loss: 0.4055 \n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8753 - loss: 0.3376 \n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8702 - loss: 0.3407 \n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9185 - loss: 0.2769 \n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9209 - loss: 0.2403 \n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8930 - loss: 0.3108 \n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8643 - loss: 0.3324 \n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8497 - loss: 0.3831 \n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9272 - loss: 0.2059 \n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9205 - loss: 0.1988 \n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9126 - loss: 0.2021 \n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9411 - loss: 0.1854 \n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9441 - loss: 0.1865 \n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9115 - loss: 0.2567 \n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8992 - loss: 0.2282 \n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9524 - loss: 0.1644 \n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9350 - loss: 0.1872 \n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9397 - loss: 0.1639 \n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9101 - loss: 0.2706 \n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8859 - loss: 0.2794 \n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9198 - loss: 0.2027 \n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9309 - loss: 0.2082 \n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9220 - loss: 0.3155 \n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9173 - loss: 0.3398 \n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8916 - loss: 0.2728 \n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9476 - loss: 0.1827 \n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9470 - loss: 0.1670 \n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9591 - loss: 0.1698 \n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9307 - loss: 0.2030 \n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9621 - loss: 0.1746 \n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9055 - loss: 0.2495 \n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9224 - loss: 0.1769 \n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8563 - loss: 0.2996 \n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8986 - loss: 0.2394 \n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9073 - loss: 0.2284 \n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9576 - loss: 0.1452 \n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - categorical_accuracy: 0.9738 - loss: 0.1484 \n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9701 - loss: 0.1199 \n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9805 - loss: 0.1056 \n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9701 - loss: 0.1210 \n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9720 - loss: 0.1084 \n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9687 - loss: 0.1170 \n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9705 - loss: 0.0916 \n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9285 - loss: 0.1993 \n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8822 - loss: 0.2671 \n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.8565 - loss: 0.4308 \n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.8938 - loss: 0.3327 \n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9025 - loss: 0.3317 \n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9504 - loss: 0.2353 \n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9497 - loss: 0.1989 \n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9067 - loss: 0.1956 \n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - categorical_accuracy: 0.9333 - loss: 0.2276 \n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9363 - loss: 0.1673 \n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - categorical_accuracy: 0.9779 - loss: 0.1077 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step\n",
      "Accuracy: 95.24%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Draw the landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        results: The landmarks detected by Mediapipe.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Draw landmarks for left hand\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "    # Draw landmarks for right hand\n",
    "    mp.solutions.drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "\n",
    "def image_process(image, model):\n",
    "    \"\"\"\n",
    "    Process the image and obtain sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        model: The Mediapipe holistic object.\n",
    "\n",
    "    Returns:\n",
    "        results: The processed results containing sign landmarks.\n",
    "    \"\"\"\n",
    "    # Set the image to read-only mode\n",
    "    image.flags.writeable = False\n",
    "    # Convert the image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image using the model\n",
    "    results = model.process(image)\n",
    "    # Set the image back to writeable mode\n",
    "    image.flags.writeable = True\n",
    "    # Convert the image back from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return results\n",
    "\n",
    "def keypoint_extraction(results):\n",
    "    \"\"\"\n",
    "    Extract the keypoints from the sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        results: The processed results containing sign landmarks.\n",
    "\n",
    "    Returns:\n",
    "        keypoints (numpy.ndarray): The extracted keypoints.\n",
    "    \"\"\"\n",
    "    # Extract the keypoints for the left hand if present, otherwise set to zeros\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    # Extract the keypoints for the right hand if present, otherwise set to zeros\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    # Concatenate the keypoints for both hands\n",
    "    keypoints = np.concatenate([lh, rh])\n",
    "    return keypoints\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from itertools import product\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Set the path to the data directory\n",
    "PATH = os.path.join(r'C:\\Harosha\\ML Projects\\Sign_Language_Detector\\Images\\Collected_Images\\Final')\n",
    "\n",
    "# Create an array of actions (signs) labels by listing the contents of the data directory\n",
    "actions = np.array(os.listdir(PATH))\n",
    "\n",
    "# Define the number of sequences and frames\n",
    "sequences = 30\n",
    "frames = 10\n",
    "\n",
    "# Create a label map to map each action label to a numeric value\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "# Initialize empty lists to store landmarks and labels\n",
    "landmarks, labels = [], []\n",
    "\n",
    "# Iterate over actions and sequences to load landmarks and corresponding labels\n",
    "\n",
    "for action, sequence in product(actions, range(sequences)):\n",
    "    temp = []\n",
    "    for frame in range(frames):\n",
    "        npy = np.load(os.path.join(PATH, action, str(sequence), str(frame) + '.npy'))\n",
    "        temp.append(npy)\n",
    "    landmarks.append(temp)\n",
    "    labels.append(label_map[action])\n",
    "\n",
    "\n",
    "# Convert landmarks and labels to numpy arrays\n",
    "X, Y = np.array(landmarks), to_categorical(labels).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=34, stratify=Y)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, activation='relu', input_shape=(10,126)))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=100)\n",
    "\n",
    "# Save the trained model with the correct extension\n",
    "model.save('my_model.keras')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "# Get the true labels from the test set\n",
    "test_labels = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = metrics.accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2ce9c-1492-4614-bf2f-e9963900c0c1",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a90512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keyboard\n",
      "  Using cached keyboard-0.13.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Using cached keyboard-0.13.5-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: keyboard\n",
      "Successfully installed keyboard-0.13.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\haros\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\cloudpickle-3.0.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\haros\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\gym-0.26.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\haros\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\gym_maze-0.4-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\haros\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\gym_notices-0.0.8-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\haros\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\pygame-2.6.0-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a49422-a7e5-4b65-b684-18ff697af7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haros\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import keyboard\n",
    "from tensorflow.keras.models import load_model\n",
    "import language_tool_python\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Draw the landmarks on the image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        results: The landmarks detected by Mediapipe.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Make sure the image is writable before drawing\n",
    "    image.flags.writeable = True\n",
    "    # Draw landmarks for left hand\n",
    "    if results.left_hand_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "    # Draw landmarks for right hand\n",
    "    if results.right_hand_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "def image_process(image, model):\n",
    "    \"\"\"\n",
    "    Process the image and obtain sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The input image.\n",
    "        model: The Mediapipe holistic object.\n",
    "\n",
    "    Returns:\n",
    "        results: The processed results containing sign landmarks.\n",
    "    \"\"\"\n",
    "    # Set the image to read-only mode\n",
    "    image.flags.writeable = False\n",
    "    # Convert the image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image using the model\n",
    "    results = model.process(image)\n",
    "    # Set the image back to writeable mode\n",
    "    image.flags.writeable = True\n",
    "    # Convert the image back from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return results\n",
    "\n",
    "def keypoint_extraction(results):\n",
    "    \"\"\"\n",
    "    Extract the keypoints from the sign landmarks.\n",
    "\n",
    "    Args:\n",
    "        results: The processed results containing sign landmarks.\n",
    "\n",
    "    Returns:\n",
    "        keypoints (numpy.ndarray): The extracted keypoints.\n",
    "    \"\"\"\n",
    "    # Extract the keypoints for the left hand if present, otherwise set to zeros\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    # Extract the keypoints for the right hand if present, otherwise set to zeros\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    # Concatenate the keypoints for both hands\n",
    "    keypoints = np.concatenate([lh, rh])\n",
    "    return keypoints\n",
    "\n",
    "# Set the path to the data directory\n",
    "#PATH = os.path.join(r'C:\\Harosha\\ML Projects\\Sign_Language_Detector\\Images\\Collected_Images\\Final')\n",
    "\n",
    "# Create an array of actions (signs) labels by listing the contents of the data directory\n",
    "#actions = np.array(os.listdir(PATH))\n",
    "actions = np.array(['Hello','Please','Sorry','No','yes','ILoveYou','Thankyou'])\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('my_model.keras')\n",
    "\n",
    "# Create an instance of the grammar correction tool\n",
    "tool = language_tool_python.LanguageToolPublicAPI('en-UK')\n",
    "\n",
    "# Initialize the lists\n",
    "sentence, keypoints, last_prediction, grammar, grammar_result = [], [], [], [], []\n",
    "\n",
    "# Access the camera and check if the camera is opened successfully\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot access camera.\")\n",
    "    exit()\n",
    "\n",
    "# Create a holistic object for sign prediction\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "    # Run the loop while the camera is open\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the camera\n",
    "        _, image = cap.read()\n",
    "        # Process the image and obtain sign landmarks using image_process function from my_functions.py\n",
    "        results = image_process(image, holistic)\n",
    "        # Draw the sign landmarks on the image using draw_landmarks function from my_functions.py\n",
    "        draw_landmarks(image, results)\n",
    "        # Extract keypoints from the pose landmarks using keypoint_extraction function from my_functions.py\n",
    "        keypoints.append(keypoint_extraction(results))\n",
    "\n",
    "        # Check if 10 frames have been accumulated\n",
    "        if len(keypoints) == 10:\n",
    "            # Convert keypoints list to a numpy array\n",
    "            keypoints = np.array(keypoints)\n",
    "            # Make a prediction on the keypoints using the loaded model\n",
    "            prediction = model.predict(keypoints[np.newaxis, :, :])\n",
    "            # Clear the keypoints list for the next set of frames\n",
    "            keypoints = []\n",
    "\n",
    "            # Check if the maximum prediction value is above 0.9\n",
    "            if np.amax(prediction) > 0.9:\n",
    "                # Check if the predicted sign is different from the previously predicted sign\n",
    "                if last_prediction != actions[np.argmax(prediction)]:\n",
    "                    # Append the predicted sign to the sentence list\n",
    "                    sentence.append(actions[np.argmax(prediction)])\n",
    "                    # Record a new prediction to use it on the next cycle\n",
    "                    last_prediction = actions[np.argmax(prediction)]\n",
    "\n",
    "        # Limit the sentence length to 7 elements to make sure it fits on the screen\n",
    "        if len(sentence) > 7:\n",
    "            sentence = sentence[-7:]\n",
    "\n",
    "        # Reset if the \"Spacebar\" is pressed\n",
    "        if keyboard.is_pressed(' '):\n",
    "            sentence, keypoints, last_prediction, grammar, grammar_result = [], [], [], [], []\n",
    "\n",
    "        # Check if the list is not empty\n",
    "        if sentence:\n",
    "            # Capitalize the first word of the sentence\n",
    "            sentence[0] = sentence[0].capitalize()\n",
    "\n",
    "        # Check if the sentence has at least two elements\n",
    "        if len(sentence) >= 2:\n",
    "            # Check if the last element of the sentence belongs to the alphabet (lower or upper cases)\n",
    "            if sentence[-1] in string.ascii_lowercase or sentence[-1] in string.ascii_uppercase:\n",
    "                # Check if the second last element of sentence belongs to the alphabet or is a new word\n",
    "                if sentence[-2] in string.ascii_lowercase or sentence[-2] in string.ascii_uppercase or (sentence[-2] not in actions and sentence[-2] not in list(x.capitalize() for x in actions)):\n",
    "                    # Combine last two elements\n",
    "                    sentence[-1] = sentence[-2] + sentence[-1]\n",
    "                    sentence.pop(len(sentence) - 2)\n",
    "                    sentence[-1] = sentence[-1].capitalize()\n",
    "\n",
    "        # Perform grammar check if \"Enter\" is pressed\n",
    "        if keyboard.is_pressed('enter'):\n",
    "            # Record the words in the sentence list into a single string\n",
    "            text = ' '.join(sentence)\n",
    "            # Apply grammar correction tool and extract the corrected result\n",
    "            grammar_result = tool.correct(text)\n",
    "\n",
    "        if grammar_result:\n",
    "            # Calculate the size of the text to be displayed and the X coordinate for centering the text on the image\n",
    "            textsize = cv2.getTextSize(grammar_result, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "            text_X_coord = (image.shape[1] - textsize[0]) // 2\n",
    "\n",
    "            # Draw the sentence on the image\n",
    "            cv2.putText(image, grammar_result, (text_X_coord, 470),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            # Calculate the size of the text to be displayed and the X coordinate for centering the text on the image\n",
    "            textsize = cv2.getTextSize(' '.join(sentence), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "            text_X_coord = (image.shape[1] - textsize[0]) // 2\n",
    "\n",
    "            # Draw the sentence on the image\n",
    "            cv2.putText(image, ' '.join(sentence), (text_X_coord, 470),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the image on the display\n",
    "        cv2.imshow('Camera', image)\n",
    "\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        # Check if the 'Camera' window was closed and break the loop\n",
    "        if cv2.getWindowProperty('Camera',cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "\n",
    "    # Release the camera and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Shut off the server\n",
    "    tool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371ae88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
